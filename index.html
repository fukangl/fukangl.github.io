<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Fukang Liu</title>
  
  <meta name="author" content="Fukang Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/fukangl.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fukang Liu</name>
                <br>
                <font size="3"><br/>Fukangl@andrew.cmu.edu</font>
              </p>
              <br/>
              <p align="justify"><font size="3">
              I am a Master's student at the <a href="https://www.sfu.ca/computing.html" target="_blank"><font size="3">School of Computing Science</font></a>, Simon Fraser University (SFU).
              </font></p>
              <p align="justify"><font size="3">
              Before joining SFU, I worked as a Senior Research Engineer at <a href="https://www.staqu.com/" target="_blank"><font size="3">Staqu Technologies</font></a>, where I designed and developed systems that revolved around Computer Vision and Deep Learning. And even earlier, during my bachelor's, I worked with <a href="https://www.isical.ac.in/~ujjwal/" target="_blank"><font size="3">Prof. Ujjwal Bhattacharya</font></a> of <a href="https://www.isical.ac.in/" target="_blank"><font size="3">Indian Statistical Institute (ISI), Kolkata</font></a> and <a href="https://www.iitr.ac.in/departments/CSE/pages/People+Faculty+Partha_Pratim_Roy.html" target="_blank"><font size="3">Prof. Partha Pratim Roy</font></a> of <a href="https://www.iitr.ac.in" target="_blank"><font size="3">Indian Institute of Technology (IIT) Roorkee</font></a> on a variety of Computer Vision research problems.
              </font></p>
              <br>
              <p style="text-align:center">
                <a href="mailto:sairajkishore13@gmail.com" target="_blank"><font size="3">Email</font></a> &nbsp|&nbsp
                <a href="https://drive.google.com/file/d/1dwvJ5dMnrE5z3u0MLt-1QdyNL8RfEPQ6/view?usp=sharing" target="_blank">CV</a> &nbsp|&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Biography</a> &nbsp|&nbsp -->
                <a href="https://scholar.google.com/citations?user=uYP3cIcAAAAJ&hl=en&authuser=2" target="_blank"><font size="3">Google Scholar</font></a> &nbsp|&nbsp
                <a href="https://www.linkedin.com/in/sairajkishore/" target="_blank"><font size="3"> LinkedIn </font></a>  &nbsp|&nbsp
                <a href="https://github.com/sairajk" target="_blank"><font size="3"> GitHub </font></a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar_2.png" target="_blank"><img style="width:100%;max-width:100%" alt="Profile Photo" src="images/avatar_2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- RESEARCH -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
              <heading><font size="5">Research Interests</font></heading>
              <p><font size="3">
                I am interested in Computer Vision, Computer Graphics, and Deep Learning in general. Particularly, understanding the world around from 2D & 3D visual data through systems that can effectively utilize the acquired knowledge and data from other similar tasks and domains, learn from data with limited or no labels, and are robust in diverse real-world scenarios.
              </font></p>
            </td>
          </tr>
        </tbody></table>

        <!-- PUBLICATIONS -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading><font size="5">Publications</font></heading>
            </td>
          </tr>
        </tbody></table>

        <script>
          function myFunction(pub_name) {
              var x = document.getElementById(pub_name);
              if (x.style.display === 'none') {
                  x.style.display = 'block';
              } else {
                  x.style.display = 'none';
              }
        }
        </script>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



          
          <!-- UNDER REVIEW - POSE ESTIMATION -->
          <!-- <tr onmouseout="underreview_0_stop()" onmouseover="underreview_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='underreview_0_image'><img src='images/Pub-ur_pose_re.jpg'></div>
                <img src='images/Pub-ur_pose_re.jpg'>
              </div>
              <script type="text/javascript">
                function underreview_0_start() {
                  document.getElementById('underreview_0_image').style.opacity = "1";
                }

                function underreview_0_stop() {
                  document.getElementById('underreview_0_image').style.opacity = "0";
                }
                underreview_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="TODO" target="_blank">
                <papertitle><font size="3">An End-to-End Framework for Unsupervised Pose Estimation of Occluded Pedestrians</font></papertitle>
              </a>
              <br>
              <font size="3">
              <a href="https://sudip.info/" target="_blank"><font size="3">Sudip Das*</font></a>, 
              <strong><font size="3">Perla Sai Raj Kishore*</font></strong>,
              <a href="https://www.isical.ac.in/~ujjwal/" target="_blank"><font size="3">Ujjwal Bhattacharya</font></a>
              <br>
              <em>Under Review</em>, 2020
              </font>
              <br>
              <p></p>              
            </td>
          </tr> -->


          <!-- ICIP 2020 - POSE ESTIMATION -->
          <tr onmouseout="icip20_0_stop()" onmouseover="icip20_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='icip20_0_image'><img src='images/Pub-icip20_pose_re.jpg'></div>
                <img src='images/Pub-icip20_pose_re.jpg'>
              </div>
              <script type="text/javascript">
                function icip20_0_start() {
                  document.getElementById('icip20_0_image').style.opacity = "1";
                }

                function icip20_0_stop() {
                  document.getElementById('icip20_0_image').style.opacity = "0";
                }
                icip20_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9191147" target="_blank">
                <papertitle><font size="3">An End-To-End Framework For Pose Estimation of Occluded Pedestrians</font></papertitle>
              </a>
              <br>
              <font size="3">
              <a href="https://sudip.info/" target="_blank"><font size="3">Sudip Das*</font></a>, 
              <strong><font size="3">Perla Sai Raj Kishore*</font></strong>,
              <a href="https://www.isical.ac.in/~ujjwal/" target="_blank"><font size="3">Ujjwal Bhattacharya</font></a>
              <br>
              <em>International Conference on Image Processing (ICIP)</em>, 2020
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('icip20_0_abs')"><font size="3">Abstract</font></a> /
              <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
              <a href="javascript:void(0);" onclick="myFunction('icip20_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="icip20_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Pose estimation in the wild is a challenging problem, particularly in situations of(i) occlusions of varying degrees, and (ii) crowded outdoor scenes. Most of the existing studies of pose estimation did not report the performance in similar situations. Moreover, pose annotations for occluded parts of the human figures have not been provided in any of the relevant standard datasets, which in turn creates further difficulties to the required studies for pose estimation of the entire Figure for occluded humans. Well known pedestrian detection datasets such as CityPersons contains samples of outdoor scenes but it does not include pose annotations. Here we propose a novel multi-task framework for end-to-end training towards the entire pose estimation of pedestrians including in situations of any kind of occlusion. To tackle this problem, we make use of a pose estimation dataset, MS-COCO, and employ unsupervised adversarial instance-level domain adaptation for estimating the entire pose of occluded pedestrians. The experimental studies show that the proposed framework outperforms the SOTA results for pose estimation, instance segmentation and pedestrian detection in cases of heavy occlusions (HO) and reasonable + heavy occlusions (R+HO) on the two benchmark datasets.
                </em>
              </font></div>
              <div id="icip20_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @INPROCEEDINGS{9191147,<br>
                  &emsp;author={S. {Das} and P. S. R. {Kishore} and U. {Bhattacharya}},<br>
                  &emsp;booktitle={2020 IEEE International Conference on Image Processing (ICIP)},<br> 
                  &emsp;title={An End-To-End Framework For Pose Estimation Of Occluded Pedestrians},<br> 
                  &emsp;year={2020},<br>
                  &emsp;volume={},<br>
                  &emsp;number={},<br>
                  &emsp;pages={1446-1450},<br>
                  &emsp;doi={10.1109/ICIP40778.2020.9191147}<br>
                }
              </font></div>
            </td>
          </tr>





          <!-- BMVC 2019 - POSE ESTIMATION -->
          <tr onmouseout="bmvc19_0_stop()" onmouseover="bmvc19_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bmvc19_0_image'><img src='images/Pub-bmvc19_pose_re.jpg'></div>
                <img src='images/Pub-bmvc19_pose_re.jpg'>
              </div>
              <script type="text/javascript">
                function bmvc19_0_start() {
                  document.getElementById('bmvc19_0_image').style.opacity = "1";
                }

                function bmvc19_0_stop() {
                  document.getElementById('bmvc19_0_image').style.opacity = "0";
                }
                bmvc19_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://bmvc2019.org/wp-content/uploads/papers/0466-paper.pdf" target="_blank">
                <papertitle><font size="3">ClueNet: A Deep Framework for Occluded Pedestrian Pose Estimation</font></papertitle>
              </a>
              <br>
              <font size="3">
              <strong><font size="3">Perla Sai Raj Kishore*</font></strong>,
              <a href="https://sudip.info/" target="_blank"><font size="3">Sudip Das*</font></a>,
              Partha Sarathi Mukherjee,
              <a href="https://www.isical.ac.in/~ujjwal/" target="_blank"><font size="3">Ujjwal Bhattacharya</font></a>
              <br>
              <em>British Machine Vision Conference (BMVC)</em>, 2019
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('bmvc19_0_abs')"><font size="3">Abstract</font></a> /
              <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
              <a href="javascript:void(0);" onclick="myFunction('bmvc19_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="bmvc19_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Pose estimation of a pedestrian helps to gather information about the current activity or the instant behaviour of the subject. Such information is useful for autonomous vehicles, augmented reality, video surveillance, etc. Although a large volume of pedestrian detection studies are available in the literature, detection of the same in situations of significant occlusions still remains a challenging task. In this work, we take a step further to propose a novel deep learning framework, called ClueNet, to detect as well as estimate the entire pose of occluded pedestrians in an unsupervised manner. ClueNet is a two stage framework where the first stage generates visual clues for the second stage to accurately estimate the pose of occluded pedestrians. The first stage employs a multi-task network to segment the visible parts and predict a bounding box enclosing the visible and occluded regions for each pedestrian. The second stage uses these predictions from the first stage for pose estimation. Here we propose a novel strategy, called Mask and Predict, to train our ClueNet to estimate the pose even for occluded regions. Additionally, we make use of various other training strategies to further improve our results. The proposed work is first of its kind and the experimental results on CityPersons and MS COCO datasets show the superior performance of our approach over existing methods.
                </em>
              </font></div>
              <div id="bmvc19_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @article{kishore2019cluenet,<br>
                  &emsp;title={ClueNet: A Deep Framework for Occluded Pedestrian Pose Estimation},<br>
                  &emsp;author={Kishore, Perla Sai Raj and Das, Sudip and Mukherjee, Partha Sarathi and Bhattacharya, Ujjwal},<br>
                  &emsp;year={2019}<br>
                }
              </font></div>
            </td>
          </tr>


 
         
        
         


          <!-- <tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/loss_after.jpg'></div>
                <img src='images/loss_before.jpg'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }

                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=1xpZ0fL9h1y9RfcTyPgVkxUrF3VwdkBvq">
                <papertitle>A General and Adaptive Robust Loss Function</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1701.03077">arxiv</a> /
              <a href="https://drive.google.com/open?id=1HNveL7xSNh6Ss7sxLK8Mw2L1Fc-rRhL4">supplement</a> /
              <a href="https://youtu.be/BmNKbnF69eY">video</a> /
              <a href="https://www.youtube.com/watch?v=4IInDT_S0ow&t=37m22s">talk</a> / 
              <a href="https://drive.google.com/file/d/1GzRYRIfLHvNLT_QwjHoBjHkBbs3Nbf0x/view?usp=sharing">slides</a> / 
              <a href="https://github.com/google-research/google-research/tree/master/robust_loss">tensorflow code</a> /
              <a href="https://github.com/jonbarron/robust_loss_pytorch">pytorch code</a> /
              <a href="data/BarronCVPR2019_reviews.txt">reviews</a> /
              <a href="data/BarronCVPR2019.bib">bibtex</a>
              <p></p>
              <p>A single robust loss function is a superset of many other common robust loss functions, and allows training to automatically adapt the robustness of its own loss.</p>
            </td>
          </tr> -->

        </tbody></table> 



        <!-- Footer - Template Credits -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template credits : 
                <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>


      </td>
    </tr>
  </table>
</body>

</html>
