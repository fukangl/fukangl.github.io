<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Fukang Liu</title>
  
  <meta name="author" content="Fukang Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/fukangl.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fukang Liu</name>
                <br>
                <font size="3"><br/>Fukangl@andrew.cmu.edu</font>
              </p>
              <br/>
              <p align="justify"><font size="3">
              I am a graduate student at Carnegie Mellon University.
              I am co-advised by Prof. <a href="https://www.ri.cmu.edu/ri-faculty/zackory-erickson/">Zackory Erikson</a> and Prof. <a href="https://www.ri.cmu.edu/ri-faculty/zeynep-temel/">Zeynep Temel</a>.
              </font></p>
              <p align="justify"><font size="3">
              Before coming to CMU, I graduated from Tsinghua University with a M.S. in Mechanics and from Tongji Unviersity with a B.S. in Engineering Mechanics.
              My research interests lie in robotics, machine learning, and reinforcement learning.
              My current research focuses on assistive robotics.  
              </font></p>
              <br>
              <p style="text-align:center">
                <a href="data/CV.pdf"  target="_blank">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=8B8iJW4AAAAJ&hl=en&oi=ao" target="_blank">Google Scholar</a> &nbsp/&nbsp                
		            <a href="https://github.com/fukangl/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar_2.png" target="_blank"><img style="width:100%;max-width:100%" alt="Profile Photo" src="images/avatar_2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- RESEARCH

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle;text-align:justify">
              <heading><font size="5">Research Interests</font></heading>
              <p><font size="3">
                I am interested in Computer Vision, Computer Graphics, and Deep Learning in general. Particularly, understanding the world around from 2D & 3D visual data through systems that can effectively utilize the acquired knowledge and data from other similar tasks and domains, learn from data with limited or no labels, and are robust in diverse real-world scenarios.
              </font></p>
            </td>
          </tr>
        </tbody></table>

         

        <!--PUBLICATIONS -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading><font size="5">Publications</font></heading>
            </td>
          </tr>
        </tbody></table>

        <script>
          function myFunction(pub_name) {
              var x = document.getElementById(pub_name);
              if (x.style.display === 'none') {
                  x.style.display = 'block';
              } else {
                  x.style.display = 'none';
              }
        }
        </script>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>




          <!-- ICIP 2020 - POSE ESTIMATION -->
          <tr onmouseout="icip20_0_stop()" onmouseover="icip20_0_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='icip20_0_image'><img src='images/Pub-icip20_pose_re.jpg'></div>
                <img src='images/Pub-icip20_pose_re.jpg'>
              </div>
              <script type="text/javascript">
                function icip20_0_start() {
                  document.getElementById('icip20_0_image').style.opacity = "1";
                }

                function icip20_0_stop() {
                  document.getElementById('icip20_0_image').style.opacity = "0";
                }
                icip20_0_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9191147" target="_blank">
                <papertitle><font size="3">An End-To-End Framework For Pose Estimation of Occluded Pedestrians</font></papertitle>
              </a>
              <br>
              <font size="3">
              <a href="https://sudip.info/" target="_blank"><font size="3">Sudip Das*</font></a>, 
              <strong><font size="3">Perla Sai Raj Kishore*</font></strong>,
              <a href="https://www.isical.ac.in/~ujjwal/" target="_blank"><font size="3">Ujjwal Bhattacharya</font></a>
              <br>
              <em>International Conference on Image Processing (ICIP)</em>, 2020
              </font>
              <br>
              <p></p>
              <a href="javascript:void(0);" onclick="myFunction('icip20_0_abs')"><font size="3">Abstract</font></a> /
              <!-- <a href="https://arxiv.org/abs/1811.01401" target="_blank"><font size="3">arXiv</font></a> / -->
              <a href="javascript:void(0);" onclick="myFunction('icip20_0_bib')"><font size="3">BibTex</font></a>
              <p></p>
              <div id="icip20_0_abs" style="display:none; text-align:justify;min-width:350px;"><font size="3">
                <em>
                  Pose estimation in the wild is a challenging problem, particularly in situations of(i) occlusions of varying degrees, and (ii) crowded outdoor scenes. Most of the existing studies of pose estimation did not report the performance in similar situations. Moreover, pose annotations for occluded parts of the human figures have not been provided in any of the relevant standard datasets, which in turn creates further difficulties to the required studies for pose estimation of the entire Figure for occluded humans. Well known pedestrian detection datasets such as CityPersons contains samples of outdoor scenes but it does not include pose annotations. Here we propose a novel multi-task framework for end-to-end training towards the entire pose estimation of pedestrians including in situations of any kind of occlusion. To tackle this problem, we make use of a pose estimation dataset, MS-COCO, and employ unsupervised adversarial instance-level domain adaptation for estimating the entire pose of occluded pedestrians. The experimental studies show that the proposed framework outperforms the SOTA results for pose estimation, instance segmentation and pedestrian detection in cases of heavy occlusions (HO) and reasonable + heavy occlusions (R+HO) on the two benchmark datasets.
                </em>
              </font></div>
              <div id="icip20_0_bib" style="font-family:Courier;display:none;min-width:350px;"><font size="2">
                <br>
                @INPROCEEDINGS{9191147,<br>
                  &emsp;author={S. {Das} and P. S. R. {Kishore} and U. {Bhattacharya}},<br>
                  &emsp;booktitle={2020 IEEE International Conference on Image Processing (ICIP)},<br> 
                  &emsp;title={An End-To-End Framework For Pose Estimation Of Occluded Pedestrians},<br> 
                  &emsp;year={2020},<br>
                  &emsp;volume={},<br>
                  &emsp;number={},<br>
                  &emsp;pages={1446-1450},<br>
                  &emsp;doi={10.1109/ICIP40778.2020.9191147}<br>
                }
              </font></div>
            </td>
          </tr>

 
         
        
         


          <!-- <tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/loss_after.jpg'></div>
                <img src='images/loss_before.jpg'>
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }

                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/open?id=1xpZ0fL9h1y9RfcTyPgVkxUrF3VwdkBvq">
                <papertitle>A General and Adaptive Robust Loss Function</papertitle>
              </a>
              <br>
              <strong>Jonathan T. Barron</strong>
              <br>
              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/1701.03077">arxiv</a> /
              <a href="https://drive.google.com/open?id=1HNveL7xSNh6Ss7sxLK8Mw2L1Fc-rRhL4">supplement</a> /
              <a href="https://youtu.be/BmNKbnF69eY">video</a> /
              <a href="https://www.youtube.com/watch?v=4IInDT_S0ow&t=37m22s">talk</a> / 
              <a href="https://drive.google.com/file/d/1GzRYRIfLHvNLT_QwjHoBjHkBbs3Nbf0x/view?usp=sharing">slides</a> / 
              <a href="https://github.com/google-research/google-research/tree/master/robust_loss">tensorflow code</a> /
              <a href="https://github.com/jonbarron/robust_loss_pytorch">pytorch code</a> /
              <a href="data/BarronCVPR2019_reviews.txt">reviews</a> /
              <a href="data/BarronCVPR2019.bib">bibtex</a>
              <p></p>
              <p>A single robust loss function is a superset of many other common robust loss functions, and allows training to automatically adapt the robustness of its own loss.</p>
            </td>
          </tr> -->

        </tbody></table> 



        <!-- Footer - Template Credits -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template credits : 
                <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>


      </td>
    </tr>
  </table>
</body>

</html>
